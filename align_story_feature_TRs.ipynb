{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)  # Set random seed\n",
    "\n",
    "# from scipy.stats import zscore\n",
    "# def pearson_corr(X,Y):\n",
    "#     return np.mean(zscore(X)*zscore(Y),0)\n",
    "\n",
    "# def compute_pearson_from_preds_numpy(pred_path, expt_setting):\n",
    "#     print('compute_pearson_from_preds_numpy:', pred_path, expt_setting)\n",
    "#     loaded = np.load(pred_path, allow_pickle=True)\n",
    "#     preds_t_per_feat = loaded.item()['preds_t']                             # (1191, ~27905)\n",
    "#     test_t_per_feat = loaded.item()['test_t']\n",
    "    \n",
    "#     if expt_setting == 'full':  # Use the full set\n",
    "#         pass\n",
    "#     else:\n",
    "#         TR_one_hot_pkl_path = f'./data/TR_one_hot_for_features/{expt_setting}.pkl'\n",
    "#         TR_one_hot = pickle.load(open(TR_one_hot_pkl_path, 'rb'))\n",
    "#         TR_indices = np.where(TR_one_hot==1)[0]                             # List of TR indices: [10, 13,...]\n",
    "#         TR_indices = np.random.choice(TR_indices, size=162, replace=False)  # Randomly select 162 TR indices, with set seed\n",
    "#         TR_indices = np.sort(TR_indices)\n",
    "        \n",
    "#         preds_t_per_feat = preds_t_per_feat[TR_indices, :]                  # Extract from only the desired TRs\n",
    "#         test_t_per_feat =  test_t_per_feat[TR_indices, :]\n",
    "    \n",
    "#     pearson_per_voxel = pearson_corr(preds_t_per_feat, test_t_per_feat)     # (~27905,)\n",
    "#     pearson_scalar = np.mean(pearson_per_voxel)                             # 1 scalar number\n",
    "#     return pearson_scalar, pearson_per_voxel\n",
    "\n",
    "# pred_path = '2-encoding_predictions/led-base/predict_F_with_led-base_layer_6_len_100.npy'\n",
    "# expt_setting = 'Characters'\n",
    "# print(compute_pearson_from_preds_numpy(pred_path, expt_setting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_path = '2-encoding_predictions/led-booksum/predict_H_with_led-booksum_layer_1_len_20.npy'\n",
    "# loaded = np.load(pred_path, allow_pickle=True)\n",
    "# preds_t_per_feat = loaded.item()['preds_t']\n",
    "# test_t_per_feat = loaded.item()['test_t']\n",
    "# print('preds:', preds_t_per_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import CV_ind\n",
    "\n",
    "test_word_indices_1 = [ 2, 20, 21, 39, 101, 132, 139, 150, 158, 188, 443, 451, 464, 569, 570, 601, 626, 633, 645, 648, 955, 956, 1222, 1244, 1272, 1298, 1306, 1316, 1319, 1321, 1343, 1347, 1352, 1356, 1369, 1500, 1504, 1515, 1523, 1552, 1556, 1591, 1593, 1604, 1612, 1615, 1796, 1811, 2667, 2690, 2696, 2702, 2704, 2708, 2746, 2752, 2755, 2777, 2792, 2794, 2805, 2822, 2895, 2919, 2942, 2987, 3093, 3144, 3162, 3831, 3883, 3895, 3898, 3930, 4218, 4230, 4249]\n",
    "\n",
    "def convert_word_indices_to_TR_one_hot(word_indices, SKIP_WORDS=20, END_WORDS=5176):   # list word indices ==> [20, 21, 39, ...]\n",
    "    time = np.load('./data/fMRI/time_fmri.npy')                 # (1351,) => [0, 2, 4, ...]\n",
    "    runs = np.load('./data/fMRI/runs_fmri.npy')                 # (1351,) => [1,...,2,...,3,...,4]\n",
    "    time_words = np.load('./data/fMRI/time_words_fmri.npy')     # (5176,) => [20., 20.5, ..., 2693.0]\n",
    "    time_words = time_words[SKIP_WORDS:END_WORDS]               # (5156,) => [30., 30.5, ..., 2693.0]\n",
    "\n",
    "    # Find what TR each word belongs to\n",
    "    words_id = np.zeros([len(time_words)],dtype=int)            # (5156,) => [0, 0, ..., 0]\n",
    "    for i in range(len(time_words)):                \n",
    "        words_id[i] = np.where(time_words[i]> time)[0][-1]      # (5156,) => [14, ..., 1346] first word belongs to TR 14\n",
    "\n",
    "    word_one_hot = np.zeros((5176))\n",
    "    word_one_hot[word_indices] = 1                              # (5176,) ==> [1, 0, 0, 1, 1, ...]\n",
    "    word_one_hot = word_one_hot[SKIP_WORDS:END_WORDS]           # (5156,) ==> [0, 1, 0, 0, 1, ...]\n",
    "    \n",
    "    TR_indices = words_id[np.where(word_one_hot == 1)]          # [14, 15, 19, ..., 35]\n",
    "    TR_one_hot = np.zeros((time.shape[0]))\n",
    "    TR_one_hot[TR_indices] = 1                                  # (1351,) ==> [0, 1, 0, 0, 1, ...]\n",
    "\n",
    "    # remove the edges of each run\n",
    "    TR_one_hot_runs = [TR_one_hot[runs==i][20:-15] for i in range(1,5)]     # (1211,) ==> IMPORTANT how 1351 --> 1211\n",
    "    TR_one_hot = np.array([])\n",
    "    for i in range(len(TR_one_hot_runs)):\n",
    "        TR_one_hot = np.concatenate([ TR_one_hot, TR_one_hot_runs[i] ])\n",
    "    \n",
    "    final_TR_one_hot = np.array([])\n",
    "    n_folds = 4\n",
    "    skip = 5\n",
    "    ind = CV_ind(len(TR_one_hot), n_folds=n_folds)              # (1211,) => [0., 0., 0., ..., 3., 3., 3.]\n",
    "    for ind_num in range(n_folds):\n",
    "        test_ind = ind==ind_num                                 # (1211,) => [ True,  True,  True, ..., False, False, False]\n",
    "        TR_one_hot_fold = np.array(TR_one_hot)[test_ind]\n",
    "        if ind_num == 0 or ind_num == n_folds-1:\n",
    "            pass\n",
    "        else:\n",
    "            TR_one_hot_fold = TR_one_hot_fold[skip:-skip]\n",
    "\n",
    "        final_TR_one_hot = np.concatenate([ final_TR_one_hot, TR_one_hot_fold ])\n",
    "    print('final_TR_one_hot shape:', final_TR_one_hot.shape[0])\n",
    "    print('final_TR_one_hot number of 1s:', len(np.where(final_TR_one_hot == 1)[0]))\n",
    "    return final_TR_one_hot\n",
    "    \n",
    "convert_word_indices_to_TR_one_hot(test_word_indices_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_text_array = np.load(os.getcwd() + '/data/stimuli_words.npy')\n",
    "\n",
    "story_features_mat = sio.loadmat('./data/story_features.mat')\n",
    "story_features = story_features_mat['features'][0]\n",
    "\n",
    "n_upper_features = len(story_features)\n",
    "for ind_upper_feature in range(0, n_upper_features):\n",
    "    upper_features = story_features[ind_upper_feature]              # Data for Speech features\n",
    "    upper_feature_name = upper_features[0][0]\n",
    "    print(f'Index {ind_upper_feature}, Upper feature name:', upper_feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_sub_feature(upper_feature_name, sub_feature_name):\n",
    "    if upper_feature_name == 'Motion':  # Strange data labeling for Motion\n",
    "        if ('sticky' in sub_feature_name) and (not sub_feature_name == 'collidePhys_sticky'):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    # Commented out because I decided to go with the original authors' annotations\n",
    "    # if upper_feature_name == 'Emotion':   \n",
    "    #     if sub_feature_name in ['commanding', 'dislike', '']:\n",
    "    #         return True\n",
    "        \n",
    "    skip_bool = True\n",
    "    skip_bool = ('sticky' in sub_feature_name)\n",
    "    \n",
    "    return skip_bool\n",
    "\n",
    "def keep_names(word_indices, HP_text_array):\n",
    "    names  = ['Draco', 'Malfoy', 'Filch', 'Harry', 'Potter', 'Hermione', 'Granger', 'Hooch', 'McGonagall']\n",
    "    names += ['Neville','Longbottom', 'Peeves', 'Ron', 'Wood', ]\n",
    "    names = [name.lower() for name in names]\n",
    "    \n",
    "    ret_indices = []\n",
    "    for ind in word_indices:\n",
    "        word = HP_text_array[ind].lower()   # Just match lowercase\n",
    "        for name in names:\n",
    "            if name in word:                # Used \"in\" so words like Harry'd've would still be included\n",
    "                ret_indices.append(ind)\n",
    "                break\n",
    "    \n",
    "    return ret_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "n_upper_features = len(story_features)\n",
    "\n",
    "for ind_upper_feature in range(1, n_upper_features):\n",
    "\n",
    "    upper_features = story_features[ind_upper_feature]              # Data for Characters features\n",
    "    upper_feature_name = upper_features[0][0]                       # Characters\n",
    "    sub_feature_name_list = [ f[0] for f in upper_features[1][0] ]  # ['draco', 'flich', 'harry', ...]\n",
    "    n_sub_features = len([ f[0] for f in upper_features[1][0] ])    # 10\n",
    "\n",
    "    out_txt_path = f'0-logs/HP_features/{upper_feature_name}.txt'\n",
    "    out_file = open(out_txt_path, 'w')\n",
    "    sys.stdout = out_file\n",
    "    \n",
    "    print('Upper feature name:', upper_feature_name)\n",
    "    print('Sub-features:', sub_feature_name_list)\n",
    "    print('n_sub_features:', n_sub_features)\n",
    "    print('Numeric array:', upper_features[2].shape)\n",
    "    print('===\\n')\n",
    "\n",
    "    if upper_feature_name == 'Word_Num':\n",
    "        continue\n",
    "    \n",
    "    upper_feature_word_indices = []\n",
    "    for ind_sub_feature in range(n_sub_features):\n",
    "        sub_feature_name = sub_feature_name_list[ind_sub_feature]\n",
    "        print('\\nSub-feature name:', sub_feature_name)\n",
    "        if skip_sub_feature(upper_feature_name, sub_feature_name):\n",
    "            print(\"=== SKIP this sub-feature ===\")\n",
    "            # continue\n",
    "        \n",
    "        sub_feature_ndarray = upper_features[2][:, ind_sub_feature]\n",
    "        print('sub_feature_ndarray:', sub_feature_ndarray.shape)\n",
    "        \n",
    "        unique_nums = np.unique(sub_feature_ndarray)\n",
    "        print('unique_nums:', unique_nums)\n",
    "        \n",
    "        \n",
    "\n",
    "        for unique_num in unique_nums:\n",
    "            print(f'--- unique_num: {unique_num} ---')\n",
    "            if unique_num == 0: # Number 0 means absence of feature, so ignore\n",
    "                continue\n",
    "            if skip_sub_feature(upper_feature_name, sub_feature_name):\n",
    "                print(\"=== SKIP this sub-feature ===\")\n",
    "            \n",
    "            word_indices = np.where( sub_feature_ndarray == unique_num )[0]\n",
    "            word_indices = word_indices.tolist()\n",
    "            if upper_feature_name == 'Characters':\n",
    "                word_indices = keep_names(word_indices, HP_text_array)\n",
    "\n",
    "            print('# of words:', len(word_indices))\n",
    "            print('word_indices:', word_indices)\n",
    "            print(HP_text_array[word_indices])\n",
    "            \n",
    "            if skip_sub_feature(upper_feature_name, sub_feature_name):\n",
    "                continue\n",
    "\n",
    "            # Add word indices to upper feature list\n",
    "            upper_feature_word_indices += word_indices\n",
    "            print('upper_feature_word_indices:', upper_feature_word_indices)\n",
    "\n",
    "    upper_feature_word_indices = list(set(upper_feature_word_indices))\n",
    "    print('upper_feature_word_indices:', len(upper_feature_word_indices))\n",
    "    print('upper_feature_word_indices:', (upper_feature_word_indices))\n",
    "    print('final words:', HP_text_array[upper_feature_word_indices])\n",
    "    \n",
    "    TR_one_hot = convert_word_indices_to_TR_one_hot(upper_feature_word_indices)\n",
    "    print('TR_one_hot:', TR_one_hot)\n",
    "    \n",
    "    TR_one_hot_pkl_path = f'./data/TR_one_hot_for_features/{upper_feature_name}.pkl'\n",
    "    with open(TR_one_hot_pkl_path, 'wb') as handle:\n",
    "        pickle.dump(TR_one_hot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create TR one-hot for Non-discourse features ===\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Finally, I also want to create a subset of TRs that are excluded from Character, Motion, Emotion\n",
    "included_TR_one_hot = np.zeros((1191,))\n",
    "for expt_setting in ['Characters', 'Emotion', 'Motion']:\n",
    "    expt_TR_one_hot_pkl_path = f'./data/TR_one_hot_for_features/{expt_setting}.pkl'\n",
    "    expt_TR_one_hot = pickle.load(open(expt_TR_one_hot_pkl_path, 'rb'))\n",
    "    included_TR_one_hot = included_TR_one_hot + expt_TR_one_hot\n",
    "    \n",
    "    print(f'{expt_setting}: {len(np.where(expt_TR_one_hot==1)[0])}')\n",
    "    \n",
    "print(f'TRs with 0 marked:', len(np.where(included_TR_one_hot==0)[0]))\n",
    "print(f'TRs with 1 marked:', len(np.where(included_TR_one_hot==1)[0]))\n",
    "print(f'TRs with 2 marked:', len(np.where(included_TR_one_hot==2)[0]))\n",
    "print(f'TRs with 3 marked:', len(np.where(included_TR_one_hot==3)[0]))\n",
    "\n",
    "# Get those \n",
    "non_discourse_TR_one_hot = (included_TR_one_hot==0).astype(int)\n",
    "print(f'TRs that are marked as non_discourse: {len(np.where(non_discourse_TR_one_hot==1)[0])}')\n",
    "\n",
    "non_discourse_TR_one_hot_pkl_path = f'./data/TR_one_hot_for_features/Non-Discourse.pkl'\n",
    "with open(non_discourse_TR_one_hot_pkl_path, 'wb') as handle:\n",
    "    pickle.dump(non_discourse_TR_one_hot, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create TR one-hot for Full set ===\n",
    "import numpy as np\n",
    "import pickle\n",
    "full_TR_one_hot = np.ones((1191,))\n",
    "\n",
    "full_TR_one_hot_pkl_path = f'./data/TR_one_hot_for_features/Full.pkl'\n",
    "with open(full_TR_one_hot_pkl_path, 'wb') as handle:\n",
    "    pickle.dump(full_TR_one_hot, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cb69ca65973f7e2df6528e8a145349717e53105ca199f9e62fc66c345652147"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
